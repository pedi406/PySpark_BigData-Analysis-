{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# `BIG DATA FUNDAMENTALS WITH PYSPARK`","metadata":{"_uuid":"bd691b4a-b999-45c9-85bb-617e64211461","_cell_guid":"632ce786-5026-4408-9f3e-70000c7364bc","trusted":true}},{"cell_type":"markdown","source":"## What is Big Data?\nBig data is a term used to refer to the study and applications of data sets that are too\ncomplex for traditional data-processing software `- wikipedia`","metadata":{"_uuid":"bd8c9c23-52ee-4c87-9fda-21e2e3bd3773","_cell_guid":"c8854a7e-6510-417b-9c32-59a9ccc7b3f4","trusted":true}},{"cell_type":"markdown","source":"## The 3 V's of Big Data\n- `Volume:` Variety and Velocity\n- `Volume:` Size of the data\n- `Variety:` Different sources & formats\n- `Velocity:` Speed of teh data","metadata":{"_uuid":"463e5470-85ef-41a8-bd6d-a02450dc1bd5","_cell_guid":"0f015721-b1a9-46b8-a5fe-07b9ffd91989","trusted":true}},{"cell_type":"markdown","source":"## Big Data concepts and Terminology\n- `Clustered computing:` Collection of resources of multiple machines\n- `Parallel computing:` Simultaneous computation\n- `Distributed computing:` Collection of nodes (networked computers) that run in parallel\n- `Batch processing:` Breaking the job into small pieces and running them on individual\nmachines\n- `Real-time processing:` Immediate processing of data","metadata":{"_uuid":"751364ef-5925-48c5-809c-aeebece4eca1","_cell_guid":"e3f57343-3d19-4793-a72c-60568db4aac9","trusted":true}},{"cell_type":"markdown","source":"NOTES: **Clustered computing** is the pooling of resources of multiple machines to complete jobs. **Parallel computing** is a type of computation in which many calculations are carried out simultaneously. A **distributed computing** involves nodes or networked computers that run jobs in parallel.","metadata":{"_uuid":"8b6385e7-0eca-4c67-9e24-6ab76a36cbd6","_cell_guid":"5d186e80-e86a-4fe9-a3e3-4ae1a54208f5","trusted":true}},{"cell_type":"markdown","source":"## Big Data processing systems\n- `Hadoop/MapReduce:` Scalable and fault tolerant framework written in Java\n    - Open SOurce\n    - Batch Processing\n- `Apache Spark:` General purpose and lightning fast cluster computing system\n    - Open source\n    - Both batch and real-time data processing","metadata":{"_uuid":"e36d093c-a6c1-4908-8eb9-d546490afeb2","_cell_guid":"c3b99cfa-f700-40a1-a955-fd87641d57a4","trusted":true}},{"cell_type":"markdown","source":"## Features of Apache Spark framework\n- Distributed cluster computing framework\n- Efficient in-memory computations for large data sets\n- Lightning fast data processing framework\n- Provides support for Java, Scala, Python, R and SQ","metadata":{"_uuid":"224c9b58-8c0e-4772-bbaa-6b12da156006","_cell_guid":"5e15a34f-4977-4876-88a1-80dabf990bde","trusted":true}},{"cell_type":"markdown","source":"## Apache Spark Components","metadata":{"_uuid":"f68e7077-2437-44c1-8fd4-0d0397ad1eaa","_cell_guid":"bcf3fa80-b47e-48fa-983d-cbc96ad51336","trusted":true}},{"cell_type":"markdown","source":"## Spark modes of deployment\n- `Local mode:` Single machine such as your laptop\n    - Local model convenient for testing, debugging and demonstration\n- `Cluster mode:` Set of pre-defined machines\n    - Good for production\n- Workfkow: Local -> Cluster\n- No Code Changes Necessary","metadata":{"_uuid":"f949b942-477c-4056-b2d2-4e206ee08f81","_cell_guid":"72102939-a823-48d4-911d-bdf82d1e0316","trusted":true}},{"cell_type":"markdown","source":"# `PySpark: Spark with Python`","metadata":{"_uuid":"56896511-9234-452a-bbd5-d519f380790c","_cell_guid":"dd9bb75b-fab1-4313-aadb-28d5e9a1310c","trusted":true}},{"cell_type":"markdown","source":"## Overview of PySpark\n- Apache Spark is written in Scala\n- To support Python with Spark, Apache Spark Community released PySpark\n- Similar computation speed and power as Scala\n- PySpark APIs are similar to Pandas and Scikit-learn","metadata":{"_uuid":"fa19da4c-80cb-4d82-bd62-ba43153f7184","_cell_guid":"0b19a848-1c07-4d3f-90d1-b18b969e75ab","trusted":true}},{"cell_type":"markdown","source":"## What is Spark shell?\n- Interactive environment for running Spark jobs\n- Helpful for fast interactive prototyping\n- Spark’s shells allow interacting with data on disk or in memory\n- Three different Spark shells:\n    - Spark-shell for Scala\n    - PySpark-shell for Python\n    - SparkR for R","metadata":{"_uuid":"5a1796ed-8000-416d-be3d-f896cc648daa","_cell_guid":"a7cf40c4-95e9-416a-b612-95f012c4e2fe","trusted":true}},{"cell_type":"markdown","source":"## PySpark shell\n- PySpark shell is the Python-based command line tool\n- PySpark shell allows data scientists interface with Spark data structures\n- PySpark shell support connecting to a cluster","metadata":{"_uuid":"ec989a1c-801d-499d-ab00-2a21e2b5041a","_cell_guid":"c80e7ac2-ed42-41fb-a453-c4f05dbb85f7","trusted":true}},{"cell_type":"markdown","source":"## Understanding `SparkContext`\n- `SparkContext` is an entry point into the world of Spark\n- An entry point is a way of connecting to Spark cluster\n- An entry point is like a key to the house\n- PySpark has a default `SparkContext` called **sc**","metadata":{"_uuid":"d4a46910-ccbd-43a2-85fd-6c2814f51ef4","_cell_guid":"067e34fd-4db7-4973-bed7-ff63e417707e","trusted":true}},{"cell_type":"markdown","source":"NOTES:\n- An **entry point** is where control is transferred from the Operating system to the provided program. \n- In simpler terms, it's like a key to your house. Without the key you cannot enter the house, similarly, without an entry point, you cannot run any PySpark jobs","metadata":{"_uuid":"2fc8a7b5-d1d6-45be-ab93-e37f1af36bfa","_cell_guid":"6993022c-c3ff-4a9e-82e5-a7b7fb259f5f","trusted":true}},{"cell_type":"markdown","source":"### Inspecting SparkContext","metadata":{"_uuid":"2d0c6fb9-42ff-4286-a067-2c19cbba2b85","_cell_guid":"7ff59022-3e13-4beb-b3c7-3956e1f718a4","trusted":true}},{"cell_type":"code","source":"#Verifying Spark \n!pip install pyspark","metadata":{"_uuid":"c95e626a-e2b8-4ba8-b934-89780477024e","_cell_guid":"77b466f7-c113-4038-a16e-4bedd3cf86de","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:41:11.555233Z","iopub.execute_input":"2022-08-22T12:41:11.555883Z","iopub.status.idle":"2022-08-22T12:41:17.687743Z","shell.execute_reply.started":"2022-08-22T12:41:11.555809Z","shell.execute_reply":"2022-08-22T12:41:17.686619Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# finding Pyspark \n!pip install findspark","metadata":{"_uuid":"b44fc42f-e8aa-46a5-9175-0cfae16f23e6","_cell_guid":"c288bbc6-f01c-4aec-8a8b-64299dd7fc55","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:41:26.241059Z","iopub.execute_input":"2022-08-22T12:41:26.241478Z","iopub.status.idle":"2022-08-22T12:41:32.392361Z","shell.execute_reply.started":"2022-08-22T12:41:26.241409Z","shell.execute_reply":"2022-08-22T12:41:32.391087Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#importing Libraries\nfrom pyspark import SparkConf\nfrom pyspark.context import SparkContext\n\n#creating SparkContext\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))","metadata":{"_uuid":"9dd0954b-704e-4229-9ef6-b83f9c3c83b3","_cell_guid":"5cc25a14-66da-423f-b0f5-0fd6a1d2e543","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:41:38.449903Z","iopub.execute_input":"2022-08-22T12:41:38.450373Z","iopub.status.idle":"2022-08-22T12:41:42.826217Z","shell.execute_reply.started":"2022-08-22T12:41:38.450299Z","shell.execute_reply":"2022-08-22T12:41:42.824949Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"`Version:` To retrieve SparkContext version","metadata":{"_uuid":"d85ecc40-8d52-4a2b-b249-240ade437ddd","_cell_guid":"2c2b9d0e-c362-4885-b446-63b3f1b519a5","trusted":true}},{"cell_type":"code","source":"sc.version","metadata":{"_uuid":"d1256716-8db3-418e-930e-f5c63a265f56","_cell_guid":"2f14ef0a-3f02-4f17-81a1-994e9bca8dfb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:41:50.633467Z","iopub.execute_input":"2022-08-22T12:41:50.634088Z","iopub.status.idle":"2022-08-22T12:41:50.644806Z","shell.execute_reply.started":"2022-08-22T12:41:50.634001Z","shell.execute_reply":"2022-08-22T12:41:50.643546Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"`Python Version:` To retrieve Python version of SparkContext","metadata":{"_uuid":"663cc614-99ea-4ab1-8b14-20594f7c5d7d","_cell_guid":"23829514-ac0a-4999-889b-66e9b15a19d3","trusted":true}},{"cell_type":"code","source":"sc.pythonVer","metadata":{"_uuid":"90ff2a37-9b17-4e5c-b2b8-68945cda25b2","_cell_guid":"2efdc9f9-2c37-45fd-bcba-05e70b6de160","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:41:58.040573Z","iopub.execute_input":"2022-08-22T12:41:58.040985Z","iopub.status.idle":"2022-08-22T12:41:58.047180Z","shell.execute_reply.started":"2022-08-22T12:41:58.040909Z","shell.execute_reply":"2022-08-22T12:41:58.046179Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"`Master:` URL of the cluster or “local” string to run in local mode of SparkContext","metadata":{"_uuid":"732135a2-641f-4ca7-90a8-babf599415f2","_cell_guid":"6e4232b4-dd50-44c0-9661-33d06711c63e","trusted":true}},{"cell_type":"code","source":"sc.master","metadata":{"_uuid":"73991a73-6a7a-46be-8caf-ce207d366837","_cell_guid":"4b06b6c8-123a-46b3-b30a-b59628bce512","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:42:01.470215Z","iopub.execute_input":"2022-08-22T12:42:01.470924Z","iopub.status.idle":"2022-08-22T12:42:01.476864Z","shell.execute_reply.started":"2022-08-22T12:42:01.470861Z","shell.execute_reply":"2022-08-22T12:42:01.476072Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Loading data in PySpark\n\n- SparkContext's `parallelize()` method","metadata":{"_uuid":"80df2670-52b0-4d8a-b69d-ab88ee33b6fe","_cell_guid":"0548abd2-d5b4-443c-b124-d5677fe9db22","tags":[],"trusted":true}},{"cell_type":"code","source":"rdd = sc.parallelize([1,2,3,4,5])","metadata":{"_uuid":"7e74042b-ed70-4422-bee3-918ca4606b05","_cell_guid":"3f76a15d-da03-4844-8796-0881fc85ff58","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:42:11.755859Z","iopub.execute_input":"2022-08-22T12:42:11.756555Z","iopub.status.idle":"2022-08-22T12:42:12.247055Z","shell.execute_reply.started":"2022-08-22T12:42:11.756499Z","shell.execute_reply":"2022-08-22T12:42:12.245841Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"- SparkContext's `textFile()` method","metadata":{"_uuid":"49d91466-c5bc-4b57-9bbc-8d47cf637a9d","_cell_guid":"af149cd0-f34a-4943-9b7c-be0a73da26a7","trusted":true}},{"cell_type":"code","source":"rdd2 = sc.textFile(\"test.txt\")","metadata":{"_uuid":"5a45c580-f712-48f8-8b77-0aee8f8e2343","_cell_guid":"35fb47a7-4b98-4bb2-bef0-e3ca5ce3f767","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:45:02.650626Z","iopub.execute_input":"2022-08-22T12:45:02.651079Z","iopub.status.idle":"2022-08-22T12:45:03.045737Z","shell.execute_reply.started":"2022-08-22T12:45:02.651019Z","shell.execute_reply":"2022-08-22T12:45:03.044558Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Use of function in python - `lambda()`, `map()`, `filter()`","metadata":{"_uuid":"a6e3c0b5-593a-42ff-8d5d-7a7d2b5ec93a","_cell_guid":"36b81e83-7246-4ac1-bd6b-a5b74995c033","trusted":true}},{"cell_type":"markdown","source":"### What are anonymous functions in Python?\n- Lambda functions are anonymous functions in Python\n- Very powerful and used in Python. Quite effiencetly with `map()` and `filter()`\n- Lambda functions create functions to be called later similar to def\n- It returns the functions without any name (i.e anonymous)\n- Inline a function definition or to defer execution of a code","metadata":{"_uuid":"5e3f9b65-73e3-44f3-a91c-a16ea5ddb2b1","_cell_guid":"5492838b-1592-4e98-bbe9-b6722a37b504","trusted":true}},{"cell_type":"markdown","source":"### Lambda function syntax","metadata":{"_uuid":"e591dae0-e25b-4aee-88b4-d1c8b145d62c","_cell_guid":"a0eed6ba-4915-4ec9-81d3-74fa6c3de768","trusted":true}},{"cell_type":"markdown","source":"- The general form of lambda functions is\n\n`>>> lambda arguments: expression`","metadata":{"_uuid":"df12c79c-6c4d-408c-adaf-6aee86e0a51d","_cell_guid":"6f5e6764-3e23-4ea0-a66d-b127a11efdd8","trusted":true}},{"cell_type":"markdown","source":"- Example of lambda function","metadata":{"_uuid":"a47afbd1-d2b3-4985-9df0-210438b66fb9","_cell_guid":"bbc54567-d796-4eb9-b694-fb17258f2370","trusted":true}},{"cell_type":"code","source":"double = lambda x: x * 2\nprint(double(3))","metadata":{"_uuid":"af4d7c67-23c0-4dfd-a401-a09575d5bfb0","_cell_guid":"a45f52c1-5d02-4e1a-b077-2a95d0ba2bfb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:45:12.252443Z","iopub.execute_input":"2022-08-22T12:45:12.252805Z","iopub.status.idle":"2022-08-22T12:45:12.259134Z","shell.execute_reply.started":"2022-08-22T12:45:12.252760Z","shell.execute_reply":"2022-08-22T12:45:12.257875Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Difference between def vs lambda functions","metadata":{"_uuid":"a128317d-82f1-4712-8f87-b8a369bfefb0","_cell_guid":"96c485c2-89e9-4dbd-82cb-f57837d842a9","trusted":true}},{"cell_type":"markdown","source":"- Python code to illustrate cube of a number","metadata":{"_uuid":"5e079596-846f-4f48-baff-2b3ea9fc2675","_cell_guid":"37842cd3-1ad9-474d-8da4-21144588efb1","trusted":true}},{"cell_type":"code","source":"#python Funtion\ndef cube(x):\n    return x ** 3\n\n#lambda function\ng = lambda x: x ** 3\n\n#displaying on console\nprint(g(10))\nprint(cube(10))","metadata":{"_uuid":"53803f98-b840-45fe-80b6-c9f56c4cc29e","_cell_guid":"12019b72-8729-4f2a-bf70-c7a657a43e94","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:45:16.028784Z","iopub.execute_input":"2022-08-22T12:45:16.029152Z","iopub.status.idle":"2022-08-22T12:45:16.035110Z","shell.execute_reply.started":"2022-08-22T12:45:16.029099Z","shell.execute_reply":"2022-08-22T12:45:16.034140Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### conclusion\n- No return statement for lambda\n- Can put lambda function anywhere","metadata":{"_uuid":"65d15f13-8d23-46d5-a839-b299ea3d4422","_cell_guid":"215d8053-625f-4b0e-a05b-4cadbf865aa5","trusted":true}},{"cell_type":"markdown","source":"### Use of Lambda function in Python - `map()`","metadata":{"_uuid":"d9f8185d-51eb-40d3-a039-42df97529454","_cell_guid":"98399859-7552-4c4b-802e-ac9e9a5b13ab","trusted":true}},{"cell_type":"markdown","source":"- `map()` function takes a function and a list and returns a new list which contains items\nreturned by that function for each item\n- General syntax of `map()`\n\n`>>> map(function, list)`","metadata":{"_uuid":"d530ea02-0a2e-4b79-8980-711066f36e06","_cell_guid":"7511ccc5-e84a-47dd-a558-bca1a8b53b38","trusted":true}},{"cell_type":"markdown","source":"- Example of `map()`","metadata":{"_uuid":"7b082a6c-1a86-43f3-a20e-680eb982bb99","_cell_guid":"518cdead-943a-45ce-b01e-a3afc6ed3e3c","trusted":true}},{"cell_type":"code","source":"items = [1, 2, 3, 4]\nlist(map(lambda x: x + 2 , items))","metadata":{"_uuid":"e045cca9-26bd-403c-8459-350ba7772749","_cell_guid":"ce0b1dbe-bf81-4a1b-85cf-b7a92d6d8e9b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:45:23.450440Z","iopub.execute_input":"2022-08-22T12:45:23.451168Z","iopub.status.idle":"2022-08-22T12:45:23.456884Z","shell.execute_reply.started":"2022-08-22T12:45:23.451106Z","shell.execute_reply":"2022-08-22T12:45:23.456166Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Use of Lambda function in python - `filter()`","metadata":{"_uuid":"35f6565c-3ea5-425b-8db7-5fc8f2171621","_cell_guid":"4dcdd9da-9be2-41d9-90f1-75c6205f5090","trusted":true}},{"cell_type":"markdown","source":"- `filter()` function takes a function and a list  and returns  a new list for which the function evaluates as `True`\n- General Syntax of `filter()`\n\n`>>> filter(function, list)`","metadata":{"_uuid":"c3aeb798-ffca-445e-a199-db67e2712e35","_cell_guid":"ae4c197b-ac1f-45cb-85fe-441a788fbaa1","trusted":true}},{"cell_type":"markdown","source":"- Example of `filter()`","metadata":{"_uuid":"63283f3a-1d0a-49fa-801e-fdd30a4b85c8","_cell_guid":"09acbf82-1a6a-4f8f-ba37-134aca82be6c","trusted":true}},{"cell_type":"code","source":"items = [1, 2, 3, 4]\nlist(filter(lambda x: (x%2 != 0), items))","metadata":{"_uuid":"2a8a509f-ee8c-4cbe-8bf7-51e3e14c7a1c","_cell_guid":"92c46eda-170f-43bd-bbd6-7ef64cc4e9da","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:45:30.576264Z","iopub.execute_input":"2022-08-22T12:45:30.576834Z","iopub.status.idle":"2022-08-22T12:45:30.583523Z","shell.execute_reply.started":"2022-08-22T12:45:30.576767Z","shell.execute_reply":"2022-08-22T12:45:30.582523Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# EXERCISE:","metadata":{"_uuid":"0ac3515a-fc66-4cec-bf1a-068035b4d23f","_cell_guid":"98d1fb99-5a1d-488c-9cfa-40d4095ca74f","trusted":true}},{"cell_type":"markdown","source":"- Print the version of SparkContext in the PySpark shell.\n- Print the Python version of SparkContext in the PySpark shell.\n- What is the master of SparkContext in the PySpark shell?","metadata":{"_uuid":"577ba22b-4870-44f6-b6c2-da4727e1fb9c","_cell_guid":"9de029bb-e8a0-4311-904d-a72d781edb26","trusted":true}},{"cell_type":"code","source":"# Print the version of SparkContext\nprint(\"The version of Spark Context in the PySpark shell is\", sc.version)\n\n# Print the Python version of SparkContext\nprint(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n\n# Print the master of SparkContext\nprint(\"The master of Spark Context in the PySpark shell is\", sc.master)","metadata":{"_uuid":"25b63bee-837a-45ff-8fd2-6bc30af47cb0","_cell_guid":"471c65a3-fa46-4708-86a2-0953aaad8816","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:45:37.378506Z","iopub.execute_input":"2022-08-22T12:45:37.379186Z","iopub.status.idle":"2022-08-22T12:45:37.386759Z","shell.execute_reply.started":"2022-08-22T12:45:37.379123Z","shell.execute_reply":"2022-08-22T12:45:37.385861Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"- Create a Python list named numb containing the numbers 1 to 100.\n- Load the list into Spark using Spark Context's parallelize method and assign it to a variable spark_data.","metadata":{"_uuid":"7f2929fb-1d57-4a15-8c5d-0ab43340c3f0","_cell_guid":"1dc4354f-4d89-44ba-b9a9-3cb59c59440d","trusted":true}},{"cell_type":"code","source":"# Create a Python list of numbers from 1 to 100 \nnumb = range(1, 101)\n\n# Load the list into PySpark  \nspark_data = sc.parallelize(numb)","metadata":{"_uuid":"c20b824d-28f6-4a8c-8100-e632e9c0496a","_cell_guid":"f89ff8bd-2f32-43da-879f-3e007fdfaccb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:45:54.619685Z","iopub.execute_input":"2022-08-22T12:45:54.620292Z","iopub.status.idle":"2022-08-22T12:45:54.629816Z","shell.execute_reply.started":"2022-08-22T12:45:54.620242Z","shell.execute_reply":"2022-08-22T12:45:54.629041Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"- Load a local text file README.md in PySpark shell.","metadata":{"_uuid":"d49013b2-024b-4384-806c-752ffb8c7e09","_cell_guid":"dd56a102-39ab-4721-a5ec-d63f7aa09b3c","trusted":true}},{"cell_type":"code","source":"# Load a local file into PySpark shell\nlines = sc.textFile(\"/kaggle/input/REMDME.md\")","metadata":{"_uuid":"1df19fcc-8138-44c6-9a4b-f9b02ed3b064","_cell_guid":"e9096491-a0b7-4bc9-8bc7-21d209dd268b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:45:59.830657Z","iopub.execute_input":"2022-08-22T12:45:59.831029Z","iopub.status.idle":"2022-08-22T12:45:59.880318Z","shell.execute_reply.started":"2022-08-22T12:45:59.830961Z","shell.execute_reply":"2022-08-22T12:45:59.879379Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"- Print my_list which is available in your environment.\n- Square each item in my_list using map() and lambda().\n- Print the result of map function.","metadata":{"_uuid":"5fef66d2-8865-4ba6-a7e1-ebd36917dc32","_cell_guid":"95730261-baa0-4b22-9bcb-71587d47e428","trusted":true}},{"cell_type":"code","source":"# list contaning 1 to 10 numbers\nmy_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n# Print my_list in the console\nprint(\"Input list is\", my_list)\n\n# Square all numbers in my_list\nsquared_list_lambda = list(map(lambda x: x**2, my_list))\n\n\n# Print the result of the map function\nprint(\"The squared numbers are\", squared_list_lambda)","metadata":{"_uuid":"81539314-fe44-42d9-affa-4d805f491fd1","_cell_guid":"5d204408-8216-458f-8c81-6e87c5a01b5b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:46:09.497696Z","iopub.execute_input":"2022-08-22T12:46:09.498074Z","iopub.status.idle":"2022-08-22T12:46:09.505209Z","shell.execute_reply.started":"2022-08-22T12:46:09.498027Z","shell.execute_reply":"2022-08-22T12:46:09.503394Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"- Print my_list2 which is available in your environment.\n- Filter the numbers divisible by 10 from my_list2 using filter() and lambda().\n- Print the numbers divisible by 10 from my_list2.","metadata":{"_uuid":"e1a36afd-aa1b-4511-a977-51dfae46808f","_cell_guid":"b3e7566e-badd-43d2-83b9-3dd22e58cdbb","trusted":true}},{"cell_type":"code","source":"# mylist2 defined \nmy_list2 = [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\n\n# Print my_list2 in the console\nprint(\"Input list is:\", my_list2)\n\n# Filter numbers divisible by 10\nfiltered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n\n# Print the numbers divisible by 10\nprint(\"Numbers divisible by 10 are:\", filtered_list)","metadata":{"_uuid":"ecee27dd-714c-4a85-b7cc-0b67f8abbf35","_cell_guid":"1e29b415-6b47-4de4-bb6f-a2bae97ac14c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-22T12:46:16.801622Z","iopub.execute_input":"2022-08-22T12:46:16.802062Z","iopub.status.idle":"2022-08-22T12:46:16.809660Z","shell.execute_reply.started":"2022-08-22T12:46:16.801971Z","shell.execute_reply":"2022-08-22T12:46:16.808789Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Good Luck :)","metadata":{"_uuid":"62c94e5d-7fd8-44f6-b6e1-5775f1976197","_cell_guid":"a4a760c8-20cf-416c-abcc-0c9a094042e0","trusted":true}}]}